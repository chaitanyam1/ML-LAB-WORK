{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_lab_9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcjG4CusI4LfNf+fipDBgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanyam1/ML-LAB-WORK/blob/master/ML_lab_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaPk0b8KT56A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx9FfFQBVTsc",
        "colab_type": "text"
      },
      "source": [
        "**Function for data loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiSL8_BUF7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloadfun(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  df.head()\n",
        "  Y = df['5'].to_numpy()\n",
        "  del df['5']\n",
        "  X=df.to_numpy()\n",
        "  return X, Y\n",
        "X, y = dataloadfun(\"mnist_train.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDpW6HiVSOI",
        "colab_type": "text"
      },
      "source": [
        "Splitting data for training & testing along with One hot encoding of training labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5gtKlCIVM0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "Labels=pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyJGHpTYcV10",
        "colab_type": "text"
      },
      "source": [
        "**Implementing using no hidden layer network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hp7SHvFVsD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Nohiddenlayer():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # To normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def form(self,layer1,layer2):\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "\n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            x=self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            prev_term=self.delta_mll(y,self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=0.2,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "\n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNBZsDacWIlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f516422d-bd55-433d-bdac-ea1a1069b20d"
      },
      "source": [
        "n=Nohiddenlayer(X_train,Labels)\n",
        "n.form(X_train,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=20)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0010255650050040868\n",
            "1 0.0014476980036195759\n",
            "2 0.0015030739678684039\n",
            "3 0.0018447371150345274\n",
            "4 0.001746492587260029\n",
            "5 0.0015375217440876127\n",
            "6 0.0013991124521912758\n",
            "7 0.0013269702766089664\n",
            "8 0.0013631281235844707\n",
            "9 0.0013554638248202378\n",
            "10 0.0012944360393033637\n",
            "11 0.001151923988799825\n",
            "12 0.0010006526647573211\n",
            "13 0.0008755286056983535\n",
            "14 0.0007674552378775848\n",
            "15 0.0006247815002051172\n",
            "16 0.0005112655185351355\n",
            "17 0.00040531044979429695\n",
            "18 0.00031111872053635973\n",
            "19 0.0002239530753592662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAXvnq_0WkX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "61b398a5-31cb-4b32-b4ba-1922e2f99690"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([321, 333, 294, 331, 357, 282, 287, 341, 208, 246]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkMQI7E0WqeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5bf37ddd-060d-4f64-d231-4ff0faa69fde"
      },
      "source": [
        "print(f\"accuracy using no hidden layer network  is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy using no hidden layer network  is 88.13333333333334 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3oXgoejfv0a",
        "colab_type": "text"
      },
      "source": [
        "**Implementing using Single layer neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yruf4M3uW0IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # To normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def form(self,layer1,layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=0.2,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfkXa__ZXHwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "507939cd-e363-4499-9aa2-3d2c9d1acbf6"
      },
      "source": [
        "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "n.form(X_train,l1)\n",
        "n.form(l1,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=20)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0008945907256392338\n",
            "1 0.0011974337274285424\n",
            "2 0.0008101879392223917\n",
            "3 0.0005318132730338474\n",
            "4 0.00025521585198597183\n",
            "5 2.422334761797235e-05\n",
            "6 3.4282714305342124e-05\n",
            "7 3.2744003776896395e-05\n",
            "8 9.642966594311391e-05\n",
            "9 0.00011416558987418997\n",
            "10 8.937211855963621e-05\n",
            "11 7.482251329828793e-05\n",
            "12 6.348910668961037e-05\n",
            "13 4.6834701016847055e-05\n",
            "14 3.794452034324882e-05\n",
            "15 3.530880025235031e-05\n",
            "16 3.321408614600961e-05\n",
            "17 3.1307599550748495e-05\n",
            "18 2.88845058914662e-05\n",
            "19 2.6283973030515965e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27Pa5OSYjtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "414ac845-e153-4016-81f5-326ce3623a48"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([302, 334, 297, 339, 316, 257, 292, 332, 246, 285]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvB2E5M0YuzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "465b554e-2065-4e1c-87fc-f572e78e912c"
      },
      "source": [
        "print(f\"accuracy using single layer neural network is is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy using single layer neural network is is 92.06666666666666 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKOYSerJgzrt",
        "colab_type": "text"
      },
      "source": [
        "**Implementing using Double layer neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_YNX4PkYzIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # To normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def form(self,layer1,layer2):\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=0.2,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7y7ddYeZTv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b75892d8-3a84-471b-bc91-130fed9c07f4"
      },
      "source": [
        "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "l2=Layer(100)\n",
        "n.form(X_train,l1)\n",
        "n.form(l1,l2)\n",
        "n.form(l2,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=20)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00047628213949905485\n",
            "1 0.0004589922935596233\n",
            "2 0.0008677947323690948\n",
            "3 0.0004338732721155683\n",
            "4 0.00035683316529711413\n",
            "5 6.388820033706552e-05\n",
            "6 0.0002494056808357392\n",
            "7 0.00032960995797553497\n",
            "8 8.495156041754026e-06\n",
            "9 4.192751807839343e-05\n",
            "10 4.216026770736523e-05\n",
            "11 2.260552563004458e-05\n",
            "12 2.063923473076422e-05\n",
            "13 6.1081017256228185e-06\n",
            "14 5.62665432013122e-06\n",
            "15 5.293623178910599e-06\n",
            "16 5.057851993500595e-06\n",
            "17 4.727389591491092e-06\n",
            "18 4.297869145890386e-06\n",
            "19 3.919658953800414e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftYOi1nZW47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a59d20d1-f236-44db-8a18-5115e66d5b6b"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([305, 326, 300, 343, 310, 260, 287, 326, 271, 272]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbWBF2s8ZZms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2e011789-ba2c-4a68-a052-245197da6c4c"
      },
      "source": [
        "print(f\"accuracy using double layered neural network  is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy using double layered neural network  is 92.7 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7nNe3MhTR2",
        "colab_type": "text"
      },
      "source": [
        "**Implementing using different activation fucntions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJLNl9E1ZfVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class Activations():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # To normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def form(self,layer1,layer2):\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "    def activation(self,name,z,derivative=False):\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=0.2,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeihHL79Zybs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "753cd5c2-2876-44cc-a53e-9ffbf8a6a463"
      },
      "source": [
        "n=Activations(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.form(X_train,l1)\n",
        "n.form(l1,l2)\n",
        "n.form(l2,Labels)\n",
        "n.train(batches=1000,lr=0.02,epoch=20)#lr changed to 0.02 because using 0.2 getting a runtime error of overflow"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0011098314150592378\n",
            "1 0.0003538822550208374\n",
            "2 0.000216792062260942\n",
            "3 0.00015251471838678918\n",
            "4 0.00015581023838559467\n",
            "5 0.0001714211367592924\n",
            "6 0.00019118234802958795\n",
            "7 0.00020481829894630283\n",
            "8 0.00020329376682443975\n",
            "9 0.00018771281975983375\n",
            "10 0.0001691228069781665\n",
            "11 0.00016008139530455264\n",
            "12 0.0001506591635644557\n",
            "13 0.0001428400701399321\n",
            "14 0.00013691428257514194\n",
            "15 0.00013126991830042286\n",
            "16 0.0001235710556334264\n",
            "17 0.00011503118524474906\n",
            "18 0.00010516672337555569\n",
            "19 9.216657768302992e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9W6a3xTZ2iH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cdf763f4-0fc2-436a-c088-75c030294c31"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([288, 335, 310, 308, 306, 263, 303, 332, 264, 291]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXyOiUbxZ6Uc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "690c150c-ab22-49bb-a95b-830ac06b0b58"
      },
      "source": [
        "print(f\"accuracy after implementing through different activation functions is is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy after implementing through different activation functions is is 89.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}